# For more info on HTTP files go to https://aka.ms/vs/httpfile
# https://learn.microsoft.com/da-dk/aspnet/core/test/http-files?view=aspnetcore-8.0
# look into https://learn.microsoft.com/da-dk/aspnet/core/web-api/http-repl/?view=aspnetcore-8.0&tabs=windows
# https://learn.microsoft.com/da-dk/aspnet/core/test/integration-tests?view=aspnetcore-8.0
#
# Questions:
# can we start the host from here?
# can we get the token from here?

@hostname=localhost
@port=7039
@token = eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiIsIng1dCI6Ii1LSTNROW5OUjdiUm9meG1lWm9YcWJIWkdldyIsImtpZCI6Ii1LSTNROW5OUjdiUm9meG1lWm9YcWJIWkdldyJ9.eyJhdWQiOiJhcGk6Ly9kYjAwZTRkYy00NzVmLTQ5NWYtODM0OS03YjkzODg0ZDZmYjUiLCJpc3MiOiJodHRwczovL3N0cy53aW5kb3dzLm5ldC9kMmI2N2M2NC00ODQ0LTQxODgtYTdkZi00Mjk0NDljZDFhZTcvIiwiaWF0IjoxNjkxMzUzMzU4LCJuYmYiOjE2OTEzNTMzNTgsImV4cCI6MTY5MTM1NzI1OCwiYWlvIjoiRTJGZ1lHaThYdmc4S09TS3NyVElKZUg4cmhjeUFBPT0iLCJhcHBpZCI6ImRiMDBlNGRjLTQ3NWYtNDk1Zi04MzQ5LTdiOTM4ODRkNmZiNSIsImFwcGlkYWNyIjoiMSIsImlkcCI6Imh0dHBzOi8vc3RzLndpbmRvd3MubmV0L2QyYjY3YzY0LTQ4NDQtNDE4OC1hN2RmLTQyOTQ0OWNkMWFlNy8iLCJvaWQiOiI4ZmViMzJhMC0yY2MyLTRiZGYtYWJiNS1lZTY5Njc1YmFkOGUiLCJyaCI6IjAuQVhvQVpIeTIwa1JJaUVHbjMwS1VTYzBhNTl6a0FOdGZSMTlKZzBsN2s0aE5iN1Y2QUFBLiIsInJvbGVzIjpbIlVzZXIuUHJvbXB0Il0sInN1YiI6IjhmZWIzMmEwLTJjYzItNGJkZi1hYmI1LWVlNjk2NzViYWQ4ZSIsInRpZCI6ImQyYjY3YzY0LTQ4NDQtNDE4OC1hN2RmLTQyOTQ0OWNkMWFlNyIsInV0aSI6IkNNUFI4YVpTNjBXRHB2S29YTDZMQUEiLCJ2ZXIiOiIxLjAifQ.sbaEW10AiQoERrWLubhidoa8YeHPWchKtER_mwpZgkuF6GwFNIMMZzJrIhFOO9kJMJwov4FtQ0dBuAeKrvRZzPoNT5AgMig1VD7o__K0iIN0APaQ1TGfcBCPbpul_NaJ5e_29bj2wv2Jjd9UhQ--iefLHV4OrLnrMO1NVclIWq4CEjv_1JeB52jrMj8Kp8TFNeo-nwMUOZ-1CLs7Z-MBId7728ENlmBmA7mgD1FpFqibNUjpsDk4BPypW16lw1StgwGZ4nc4MdrJCcsRWbFSI_ms7--qQW_OyNY43uxpkPYP2F5bd-eoybSXl6XabUp8L38XZeSB80R4zRB0gCGl-A


#get health status from anonymous healthcontroller
GET https://{{hostname}}:{{port}}/health HTTP/1.1


###

#get prompt template
GET https://{{hostname}}:{{port}}/api/llama/configuration/prompt-templates HTTP/1.1
Authorization: Bearer {{token}}

###

#get llama model options
GET https://{{hostname}}:{{port}}/api/llama/configuration/modelparams  HTTP/1.1
Authorization: Bearer {{token}}


###

#puv llama model options
PUT https://{{hostname}}:{{port}}/api/llama/configuration/modelparams  HTTP/1.1
Authorization: Bearer {{token}}
Content-Type: application/json

{
  "contextSize": 512,
  "gpuLayerCount": 20,
  "seed": 1337,
  "useFp16Memory": true,
  "useMemorymap": true,
  "useMemoryLock": false,
  "perplexity": false,
  "loraAdapter": "",
  "loraBase": "",
  "threads": 6,
  "batchSize": 512,
  "convertEosToNewLine": false,
  "embeddingMode": false,
  "modelName": "llama-2-7b.ggmlv3.q8_0.bin",
  "antiPrompt": null,
  "prompt": null
}

###

#get inference options
GET https://{{hostname}}:{{port}}/api/llama/configuration/inference HTTP/1.1
Authorization: Bearer {{token}}


###

#put inference options
PUT https://{{hostname}}:{{port}}/api/llama/configuration/inference HTTP/1.1
Authorization: Bearer {{token}}
Content-Type: application/json
accept: */*

{
  "tokensKeep": 0,
  "maxTokens": -1,
  "logitBias": null,
  "antiPrompts": [
    "User:"
  ],
  "pathSession": "",
  "inputSuffix": "",
  "inputPrefix": "",
  "topK": 40,
  "topP": 0.95,
  "tfsZ": 1,
  "typicalP": 1,
  "temperature": 1.0,
  "repeatPenalty": 1.1,
  "repeatLastTokensCount": 64,
  "frequencyPenalty": 0,
  "presencePenalty": 0,
  "mirostat": 0,
  "mirostatTau": 5,
  "mirostatEta": 0.1,
  "penalizeNL": true
}



###

#post chat
POST https://{{hostname}}:{{port}}/api/llama/chat HTTP/1.1
Authorization: Bearer {{token}}
Content-Type: application/json
accept: */*

{
  "modelOptions": {
    "contextSize": 1024,
    "antiPrompt": [
      "USER:"
    ],
    "prompt": "You are a very helpfull assitant"
  },
  "text": "Today is monday. What day is it tomorrow",
  "usePersistedModelState": false,
  "useDefaultAntiPrompt": false,
  "useDefaultPrompt": false
}

###

#post chat
POST https://{{hostname}}:{{port}}/api/llama/chat/stream HTTP/1.1
Authorization: Bearer {{token}}
Content-Type: application/json
accept: */*

{
  "modelOptions": {
    "contextSize": 0,
    "mainGpu": 0,
    "lowVram": true,
    "gpuLayerCount": 0,
    "seed": 0,
    "useFp16Memory": true,
    "useMemorymap": true,
    "useMemoryLock": true,
    "perplexity": true,
    "modelPath": "string",
    "modelAlias": "string",
    "loraAdapter": "string",
    "loraBase": "string",
    "threads": 0,
    "batchSize": 0,
    "convertEosToNewLine": true,
    "embeddingMode": true,
    "tensorSplits": [
      0
    ],
    "antiPrompt": [
      "string"
    ],
    "prompt": "string"
  },
  "text": "string",
  "usePersistedModelState": true,
  "useDefaultAntiPrompt": true,
  "useDefaultPrompt": true
}

###


#get tokenized
POST https://{{hostname}}:{{port}}/api/llama/tokenize HTTP/1.1
Authorization: Bearer {{token}}
Content-Type: application/json
accept: */*

{
  "modelOptions": {
    "contextSize": 0,
    "mainGpu": 0,
    "lowVram": true,
    "gpuLayerCount": 0,
    "seed": 0,
    "useFp16Memory": true,
    "useMemorymap": true,
    "useMemoryLock": true,
    "perplexity": true,
    "modelPath": "string",
    "modelAlias": "string",
    "loraAdapter": "string",
    "loraBase": "string",
    "threads": 0,
    "batchSize": 0,
    "convertEosToNewLine": true,
    "embeddingMode": true,
    "tensorSplits": [
      0
    ],
    "antiPrompt": [
      "string"
    ],
    "prompt": "string"
  },
  "text": "hello world",
  "usePersistedModelState": true
}





###

#get tokenized
POST https://{{hostname}}:{{port}}/api/llama/detokenize HTTP/1.1
Authorization: Bearer {{token}}
Content-Type: application/json
accept: */*

{
  "modelOptions": {
    "contextSize": 1024,
    "embeddingMode": true,
    "tensorSplits": [
      0
    ]  
  },
  "tokens": [  1,  12199,  3186]
}

###

#get embeddings
POST https://{{hostname}}:{{port}}/api/llama/embeddings HTTP/1.1
Authorization: Bearer {{token}}
Content-Type: application/json
accept: */*

{
  "modelOptions": {
    "contextSize": 1024
  },
  "text": "Hello to the AI world",
  "usePersistedModelState": false
}


###

#get executor
POST https://{{hostname}}:{{port}}/api/llama/executor HTTP/1.1
Authorization: Bearer {{token}}
Content-Type: application/json
accept: */*

{
  "modelOptions": {
    "contextSize": 0,
    "mainGpu": 0,
    "lowVram": true,
    "gpuLayerCount": 0,
    "seed": 0,
    "useFp16Memory": true,
    "useMemorymap": true,
    "useMemoryLock": true,
    "perplexity": true,
    "modelPath": "string",
    "modelAlias": "string",
    "loraAdapter": "string",
    "loraBase": "string",
    "threads": 0,
    "batchSize": 0,
    "convertEosToNewLine": true,
    "embeddingMode": true,
    "tensorSplits": [
      0
    ],
    "antiPrompt": [
      "string"
    ],
    "prompt": "string"
  },
  "text": "string",
  "inferenceOptions": {
    "tokensKeep": 0,
    "maxTokens": 0,
    "logitBias": {
      "additionalProp1": 0,
      "additionalProp2": 0,
      "additionalProp3": 0
    },
    "antiPrompts": [
      "string"
    ],
    "pathSession": "string",
    "inputSuffix": "string",
    "inputPrefix": "string",
    "topK": 0,
    "topP": 0,
    "tfsZ": 0,
    "typicalP": 0,
    "temperature": 0,
    "repeatPenalty": 0,
    "repeatLastTokensCount": 0,
    "frequencyPenalty": 0,
    "presencePenalty": 0,
    "mirostat": "Disable",
    "mirostatTau": 0,
    "mirostatEta": 0,
    "penalizeNL": true
  },
  "inferenceType": "InteractiveExecutor",
  "usePersistedModelState": true,
  "usePersistedExecutorState": true,
  "useStatelessExecutor": true
}


