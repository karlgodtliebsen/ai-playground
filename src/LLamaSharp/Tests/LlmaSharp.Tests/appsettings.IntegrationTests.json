{
  "LlmaModelOptions": {
    "modelPath": "LlmaModels\\wizardLM-7B.ggmlv3.q4_1.bin",
    //"contextSize": 512,
    //"gpuLayerCount": 512,
    //"seed": 512,
    //"useFp16Memory": true,
    //"useMemorymap": true,
    //"useMemoryLock": false,
    //"perplexity": false,
    //"loraAdapter": "",
    //"loraBase": "",
    //"threads": -1,
    //"batchSize": 512,
    //"convertEosToNewLine": false,
    //"embeddingMode": false
  }
}
